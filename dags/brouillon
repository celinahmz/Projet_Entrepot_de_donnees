from datetime import datetime
import os
import pandas as pd
from airflow import DAG
from airflow.operators.python_operator import PythonOperator
from airflow.hooks.postgres_hook import PostgresHook
import json
from airflow.operators.postgres_operator import PostgresOperator
from airflow.operators.email_operator import EmailOperator
import numpy as np
# Utilisez l'image de base d'Airflow
# FROM apache/airflow:latest

# # Installez les dépendances nécessaires, y compris matplotlib
# USER root
# RUN apt-get update && \
#     apt-get install -y python3-matplotlib && \
#     apt-get autoremove -y && \
#     apt-get clean && \
#     rm -rf /var/lib/apt/lists/*
# USER airflow



def extract_data():
#Fichier Urgences
    # Spécifiez le chemin vers le fichier CSV
    csv_file_path_urgences = os.path.expandvars("${AIRFLOW_HOME}/data/donnees-urgences-SOS-medecins.csv")
    # Chargez les données depuis le fichier CSV
    df_urgences = pd.read_csv(csv_file_path_urgences,  delimiter=';' ) 
    df_urgences.columns = df_urgences.columns.str.strip() # supp des espaces 
    df_urgences = df_urgences.drop_duplicates().reset_index(drop=True) #supp des lignes en doublons :
    columns_to_convert = ['dep', 'sursaud_cl_age_corona', 'nbre_pass_corona', 'nbre_pass_tot', 'nbre_hospit_corona',
                        'nbre_pass_corona_h', 'nbre_pass_corona_f', 'nbre_pass_tot_h', 'nbre_pass_tot_f',
                       'nbre_hospit_corona_h', 'nbre_hospit_corona_f', 'nbre_acte_corona', 'nbre_acte_tot',
                      'nbre_acte_corona_h', 'nbre_acte_corona_f', 'nbre_acte_tot_h', 'nbre_acte_tot_f']
    df_urgences[columns_to_convert] = df_urgences[columns_to_convert].apply(pd.to_numeric, errors='coerce')
    df_urgences[columns_to_convert] = df_urgences[columns_to_convert].fillna(0)  
     #Remplacer df_urgences['date_de_passage'] par l'accès à la première colonne par position
    df_urgences[df_urgences.columns[1]] = pd.to_datetime(df_urgences[df_urgences.columns[1]], errors='coerce')
     #pour le nettoyages des dates soit on va les supprimer ou on va  les laisser mais avec une date specifique : 
     #On voudrait bien garder le maximum des données donc pour les dates erronées on creer une date unique qui est : 1999-01-01
    df_urgences['date_de_passage'].fillna('1999-01-01', inplace=True) 


#Fichier departements 
    json_file_path_departements = os.path.expandvars("${AIRFLOW_HOME}/data/departements-region.json")
    with open(json_file_path_departements, 'r',encoding="UTF-8") as json_file:
        data = json.load(json_file)
        df_departements = pd.DataFrame(data)
        #changer le type des colonnes de departements
# Utiliser pd.to_numeric avec errors='coerce' pour remplacer les valeurs non convertissables par NaN
        df_departements['num_dep'] = pd.to_numeric(df_departements['num_dep'], errors='coerce')

# Remplacer les NaN par une valeur par défaut si nécessaire
# df_departements['num_dep'] = df_departements['num_dep'].fillna(default_value)

# Convertir la colonne en type entier
        df_departements['num_dep'] = df_departements['num_dep'].astype('Int64')
        df_departements['dep_name'] = df_departements['dep_name'].astype(str)
        df_departements['dep_name'] = df_departements['region_name'].astype(str)
        #print("jawek  <------------------------------------->    zwine ")
        print(len(df_departements))
        df_departements = df_departements.dropna(subset=['num_dep'])
        print(len(df_departements))
# Suppose df est votre DataFrame
# Remplacez df par le nom de votre DataFrame si nécessaire

# Supprimer les lignes où la colonne "num dep" est vide


# Si vous souhaitez effectuer la modification directement dans le DataFrame existant sans créer un nouveau DataFrame, vous pouvez utiliser l'argument inplace=True :
# df.dropna(subset=['num dep'], inplace=True)

# Maintenant, df ne contient que les lignes où la colonne "num dep" n'est pas vide

#FICHIER Age
    csv_file_path_age = os.path.expandvars("${AIRFLOW_HOME}/data/code-tranches-dage-donnees-urgences.csv")
    df_age = pd.read_csv(csv_file_path_age,  delimiter=";") 
    #changer le type des colonnes de departements
    df_age["Code tranches d'age"] = df_age["Code tranches d'age"].astype(int)
    df_age['Age'] = df_age['Age'].astype(str)
    print(df_age)
    # Remplacer toutes les valeurs égales à 0 par 1 dans les colonnes de dep
    #df_urgences['dep'] = df_urgences['dep'].replace(0, 1)


# #Alimentation de l'entrepôt s
#     postgres_sql_upload = PostgresHook(postgres_conn_id="postgres_connexion")
#     df_departements.to_sql('Departements', postgres_sql_upload.get_sqlalchemy_engine(), if_exists='replace', chunksize=1000)
#     # Supprimer la contrainte de clé étrangère d'abord
#     postgres_sql_upload.run("ALTER TABLE Urgences DROP CONSTRAINT IF EXISTS Urgences_dep_fkey")
#     # Supprimer la table avec l'option CASCADE
#     postgres_sql_upload.run("DROP TABLE IF EXISTS Departements CASCADE")
#     # Ensuite, recréer la table
#     df_departements.to_sql('Departements', postgres_sql_upload.get_sqlalchemy_engine(), if_exists='replace', chunksize=1000, method='multi', index=False)

#     #//df_departements.to_sql('Departements', postgres_sql_upload.get_sqlalchemy_engine(), if_exists='replace', chunksize=1000, method='multi', index=False)


    
#     postgres_sql_upload = PostgresHook(postgres_conn_id="postgres_connexion")
#     df_age.to_sql('Age', postgres_sql_upload.get_sqlalchemy_engine(), if_exists='replace', chunksize=1000)
#     #df_age.to_sql('Age', postgres_sql_upload.get_sqlalchemy_engine(), if_exists='replace', chunksize=1000, method='multi', index=False)
#     postgres_sql_upload = PostgresHook(postgres_conn_id="postgres_connexion")
#     df_urgences.to_sql('Urgences', postgres_sql_upload.get_sqlalchemy_engine(), if_exists='replace', chunksize=1000)
#     #df_urgences.to_sql('Urgences', postgres_sql_upload.get_sqlalchem zy_engine(), if_exists='replace', chunksize=1000, method='multi', index=False)
    print(df_age.columns)
    df_age.rename(columns={'Code tranches d\'age': 'code_age'}, inplace=True)
    df_age.rename(columns={'Age': 'agess'}, inplace=True)
    print(df_age.columns)


    print('zmmmmmmarr ')
    print(df_urgences.columns)
    # Alimentation de l'entrepôt
    postgres_sql_upload = PostgresHook(postgres_conn_id="postgres_connexion")

    # Chargement des données dans la table 'Departements'
    df_departements.to_sql('Departements', postgres_sql_upload.get_sqlalchemy_engine(), if_exists='append', chunksize=1000, index=False)

    # # Supprimer la table 'Departements' avec l'option CASCADE
    # postgres_sql_upload.run("DROP TABLE IF EXISTS Urgences cascade ")
    # postgres_sql_upload.run("DROP TABLE IF EXISTS Departements CASCADE")

    # # Recréer la table 'Departements'
    # df_departements.to_sql('Departements', postgres_sql_upload.get_sqlalchemy_engine(), if_exists='replace', chunksize=1000, method='multi', index=False)

    # Chargement des données dans la table 'Age'
    df_age.to_sql('Ages', postgres_sql_upload.get_sqlalchemy_engine(), if_exists='append', chunksize=1000, index=False)

    # Chargement des données dans la table 'Urgences'
    df_urgences.to_sql('Urgences', postgres_sql_upload.get_sqlalchemy_engine(), if_exists='append', chunksize=1000, index=False)
# Alimentation de l'entrepôt

# # Connexion à la base de données PostgreSQL
 # Create boxplot 
    # data = np.arange(1,20)
    # fig, ax = plt.subplots(figsize=(10,10)) 
    # plt.title('Title', fontsize=15)
    # plt.boxplot(x=data)       
    # plt.savefig('boxplot_test.png')
    # plt.close(fig)

    # print('Visualisations have been created.')

#     postgres_sql_upload = PostgresHook(postgres_conn_id="postgres_connexion")


#     # Suppression de toutes les lignes de la table "Age" sans supprimer la table elle-même
#     postgres_sql_upload.run("TRUNCATE TABLE Age RESTART IDENTITY CASCADE")


#     # Chargement des données dans la table 'Departements'
#     df_departements.to_sql('Departements', postgres_sql_upload.get_sqlalchemy_engine(), if_exists='append', chunksize=1000, index=False)

#     # Chargement des données dans la table 'Age'
#     df_age.to_sql('Age', postgres_sql_upload.get_sqlalchemy_engine(), if_exists='replace', chunksize=1000, index=False)

#     # Chargement des données dans la table 'Urgences'
#     df_urgences.to_sql('Urgences', postgres_sql_upload.get_sqlalchemy_engine(), if_exists='replace', chunksize=1000, index=False)
# Alimentation de la table "Departements"
    #postgres_sql_upload = PostgresHook(postgres_conn_id="postgres_connexion")
    #df_departements.to_sql('Departements', postgres_sql_upload.get_sqlalchemy_engine(), if_exists='replace', chunksize=1000, method='multi', index=False)
#
    ## Alimentation de la table "Age"
    #postgres_sql_upload = PostgresHook(postgres_conn_id="postgres_connexion")
    #df_age.to_sql('Age', postgres_sql_upload.get_sqlalchemy_engine(), if_exists='replace', chunksize=1000)
#
    ## Alimentation de la table "Urgences"
    #postgres_sql_upload = PostgresHook(postgres_conn_id="postgres_connexion")
    #df_urgences.to_sql('Urgences', postgres_sql_upload.get_sqlalchemy_engine(), if_exists='replace', chunksize=1000)
#
    ## Supprimer la contrainte de clé étrangère maintenant
    #postgres_sql_upload.run("ALTER TABLE Urgences DROP CONSTRAINT IF EXISTS Urgences_dep_fkey")



    
    
default_args = {
    'owner': 'airflow',
    'depends_on_past': False
}

with DAG(
    'Projet1',
    default_args=default_args,
    schedule_interval=None,
    start_date=datetime(2021, 1, 1),
    catchup=False,
) as dag:
    extract = PythonOperator(
        task_id='Extract',
        python_callable=extract_data
    )

    create_table = PostgresOperator(
    task_id='create_table',
    postgres_conn_id='postgres_connexion',
    sql='sql/create_table.sql'
    )
   
    # join_table = PostgresOperator(
    # task_id='join_table',
    # postgres_conn_id='postgres_connexion',
    # sql='sql/join_table.sql'
    # )

    # Exemple de tâche EmailOperator (envoie un e-mail en cas d'échec)
    # email_on_failure = EmailOperator(
    # task_id='send_email_on_failure',
    # to='celinahmz@gmail.com',
    # subject='Échec du DAG',
    # html_content='Le DAG a échoué. Veuillez vérifier les journaux.',
    # dag=dag,
    # trigger_rule='all_failed',  # Cette tâche s'exécutera uniquement en cas d'échec de toutes les tâches précédentes
    # )

    # Exemple de tâche EmailOperator (envoie un e-mail en cas de réussite)
    # email_on_success = EmailOperator(
    # task_id='send_email_on_success',
    # to='celinahmz@gmail.com',
    # subject='Succès du DAG',
    # html_content='Le DAG a réussi.',
    # dag=dag,
    # trigger_rule='all_success',  # Cette tâche s'exécutera uniquement en cas de réussite de toutes les tâches précédentes
    # )

    



create_table >> extract   #>> join_table

